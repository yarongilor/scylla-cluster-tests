# scylla-cluster-tests configuration options
| Parameter | Description  | Default | Override environment<br>variable
| :-------  | :----------  | :------ | :-------------------------------
| **<a name="config_files">config_files</a>**  | a list of config files that would be used | N/A | SCT_CONFIG_FILES
| **<a name="cluster_backend">cluster_backend</a>**  | backend that will be used, aws/gce/docker | N/A | SCT_CLUSTER_BACKEND
| **<a name="test_duration">test_duration</a>**  | Test duration (min). Parameter used to keep instances produced by tests that are<br>supposed to run longer than 24 hours from being killed | N/A | SCT_TEST_DURATION
| **<a name="n_db_nodes">n_db_nodes</a>**  | Number list of database nodes in multiple data centers. | N/A | SCT_N_DB_NODES
| **<a name="n_test_oracle_db_nodes">n_test_oracle_db_nodes</a>**  | Number list of oracle test nodes in multiple data centers. | N/A | SCT_N_TEST_ORACLE_DB_NODES
| **<a name="n_loaders">n_loaders</a>**  | Number list of loader nodes in multiple data centers | N/A | SCT_N_LOADERS
| **<a name="n_monitor_nodes">n_monitor_nodes</a>**  | Number list of monitor nodes in multiple data centers | N/A | SCT_N_MONITORS_NODES
| **<a name="intra_node_comm_public">intra_node_comm_public</a>**  | If True, all communication between nodes are via public addresses | N/A | SCT_INTRA_NODE_COMM_PUBLIC
| **<a name="endpoint_snitch">endpoint_snitch</a>**  | The snitch class scylla would use<br><br>'GossipingPropertyFileSnitch' - default<br>'Ec2MultiRegionSnitch' - default on aws backend<br>'GoogleCloudSnitch' | N/A | SCT_ENDPOINT_SNITCH
| **<a name="user_credentials_path">user_credentials_path</a>**  | Path to your user credentials. qa key are downloaded automatically from S3 bucket | N/A | SCT_USER_CREDENTIALS_PATH
| **<a name="cloud_credentials_path">cloud_credentials_path</a>**  | Path to your user credentials. qa key are downloaded automatically from S3 bucket | ~/.ssh/support | SCT_CLOUD_CREDENTIALS_PATH
| **<a name="cloud_prom_bearer_token">cloud_prom_bearer_token</a>**  | scylla cloud promproxy bearer_token to federate monitoring data into our monitoring instance | N/A | SCT_CLOUD_PROM_BEARER_TOKEN
| **<a name="cloud_prom_path">cloud_prom_path</a>**  | scylla cloud promproxy path to federate monitoring data into our monitoring instance | N/A | SCT_CLOUD_PROM_PATH
| **<a name="cloud_prom_host">cloud_prom_host</a>**  | scylla cloud promproxy hostname to federate monitoring data into our monitoring instance | N/A | SCT_CLOUD_PROM_HOST
| **<a name="ip_ssh_connections">ip_ssh_connections</a>**  | Type of IP used to connect to machine instances.<br>This depends on whether you are running your tests from a machine inside<br>your cloud provider, where it makes sense to use 'private', or outside (use 'public')<br><br>Default: Use public IPs to connect to instances (public)<br>Use private IPs to connect to instances (private)<br>Use IPv6 IPs to connect to instances (ipv6) | private | SCT_IP_SSH_CONNECTIONS
| **<a name="scylla_repo">scylla_repo</a>**  | Url to the repo of scylla version to install scylla | N/A | SCT_SCYLLA_REPO
| **<a name="scylla_version">scylla_version</a>**  | Version of scylla to install, ex. '2.3.1'<br>Automatically lookup AMIs and repo links for formal versions.<br>WARNING: can't be used together with 'scylla_repo' or 'ami_id_db_scylla' | N/A | SCT_SCYLLA_VERSION
| **<a name="oracle_scylla_version">oracle_scylla_version</a>**  | Version of scylla to use as oracle cluster with gemini tests, ex. '3.0.11'<br>Automatically lookup AMIs for formal versions.<br>WARNING: can't be used together with 'ami_id_db_oracle' | N/A | SCT_ORACLE_SCYLLA_VERSION
| **<a name="scylla_linux_distro">scylla_linux_distro</a>**  | The distro name and family name to use [centos/ubuntu-xenial/debien-jessie] | centos | SCT_SCYLLA_LINUX_DISTRO
| **<a name="scylla_linux_distro_loader">scylla_linux_distro_loader</a>**  | The distro name and family name to use [centos/ubuntu-xenial/debien-jessie] | centos | SCT_SCYLLA_LINUX_DISTRO_LOADER
| **<a name="scylla_repo_m">scylla_repo_m</a>**  | Url to the repo of scylla version to install scylla from for managment tests | http://repositories.scylladb.com/scylla/repo/qa-test/centos/scylladb-2019.1.repo | SCT_SCYLLA_REPO_M
| **<a name="scylla_repo_loader">scylla_repo_loader</a>**  | Url to the repo of scylla version to install c-s for loader | N/A | SCT_SCYLLA_REPO_LOADER
| **<a name="scylla_mgmt_repo">scylla_mgmt_repo</a>**  | Url to the repo of scylla manager version to install for management tests | https://repositories.scylladb.com/scylla/repo/qa-test/centos/scylladb-manager-2.0.repo | SCT_SCYLLA_MGMT_REPO
| **<a name="scylla_mgmt_agent_repo">scylla_mgmt_agent_repo</a>**  | Url to the repo of scylla manager agent version to install for management tests | N/A | SCT_SCYLLA_MGMT_AGENT_REPO
| **<a name="scylla_mgmt_pkg">scylla_mgmt_pkg</a>**  | Url to the scylla manager packages to install for management tests | N/A | SCT_SCYLLA_MGMT_PKG
| **<a name="use_mgmt">use_mgmt</a>**  | When define true, will install scylla management | N/A | SCT_USE_MGMT
| **<a name="mgmt_port">mgmt_port</a>**  | The port of scylla management | 10090 | SCT_MGMT_PORT
| **<a name="mgmt_segments_per_repair">mgmt_segments_per_repair</a>**  | the number of segments per repair used for the manager's repair | 10 | MGMT_SEGMENTS_PER_REPAIR
| **<a name="update_db_packages">update_db_packages</a>**  | A local directory of rpms to install a custom version on top of<br>the scylla installed (or from repo or from ami) | N/A | SCT_UPDATE_DB_PACKAGES
| **<a name="monitor_branch">monitor_branch</a>**  | The port of scylla management | branch-3.1 | SCT_MONITOR_BRANCH
| **<a name="db_type">db_type</a>**  | Db type to install into db nodes, scylla/cassandra | scylla | SCT_DB_TYPE
| **<a name="user_prefix">user_prefix</a>**  | the prefix of the name of the cloud instances, defaults to username | N/A | SCT_USER_PREFIX
| **<a name="ami_id_db_scylla_desc">ami_id_db_scylla_desc</a>**  | version name to report stats to Elasticsearch and tagged on cloud instances | N/A | SCT_AMI_ID_DB_SCYLLA_DESC
| **<a name="store_results_in_elasticsearch">store_results_in_elasticsearch</a>**  | save the results in elasticsearch | True | SCT_STORE_RESULTS_IN_ELASTICSEARCH
| **<a name="sct_public_ip">sct_public_ip</a>**  | Override the default hostname address of the sct test runner,<br>for the monitoring of the Nemesis.<br>can only work out of the box in AWS | N/A | SCT_SCT_PUBLIC_IP
| **<a name="sct_ngrok_name">sct_ngrok_name</a>**  | Override the default hostname address of the sct test runner,<br>using ngrok server, see readme for more instructions | N/A | SCT_NGROK_NAME
| **<a name="backtrace_decoding">backtrace_decoding</a>**  | If True, all backtraces found in db nodes would be decoded automatically | True | SCT_BACKTRACE_DECODING
| **<a name="instance_provision">instance_provision</a>**  | instance_provision: on_demand|spot_fleet|spot_low_price|spot_duration | spot_low_price | SCT_INSTANCE_PROVISION
| **<a name="reuse_cluster">reuse_cluster</a>**  | If reuse_cluster is set it should hold test_id of the cluster that will be reused.<br>`reuse_cluster: 7dc6db84-eb01-4b61-a946-b5c72e0f6d71` | N/A | SCT_REUSE_CLUSTER
| **<a name="test_id">test_id</a>**  | test id to filter by | N/A | SCT_TEST_ID
| **<a name="seeds_selector">seeds_selector</a>**  | How to select the seeds. Expected values: reflector/random/first | first | SCT_SEEDS_SELECTOR
| **<a name="seeds_num">seeds_num</a>**  | Number of seeds to select | 1 | SCT_SEEDS_NUM
| **<a name="send_email">send_email</a>**  | If true would send email out of the performance regression test | N/A | SCT_SEND_EMAIL
| **<a name="email_recipients">email_recipients</a>**  | list of email of send the performance regression test to | ['qa@scylladb.com'] | SCT_EMAIL_RECIPIENTS
| **<a name="bench_run">bench_run</a>**  | If true would kill the scylla-bench thread in the test teardown | N/A | SCT_BENCH_RUN
| **<a name="fullscan">fullscan</a>**  | If true would kill the fullscan thread in the test teardown | N/A | SCT_FULLSCAN
| **<a name="experimental">experimental</a>**  | when enabled scylla will use it's experimental features | True | SCT_EXPERIMENTAL
| **<a name="server_encrypt">server_encrypt</a>**  | when enable scylla will use encryption on the server side | N/A | SCT_SERVER_ENCRYPT
| **<a name="client_encrypt">client_encrypt</a>**  | when enable scylla will use encryption on the client side | N/A | SCT_CLIENT_ENCRYPT
| **<a name="hinted_handoff">hinted_handoff</a>**  | when enable or disable scylla hinted handoff (enabled/disabled) | N/A | SCT_HINTED_HANDOFF
| **<a name="authenticator">authenticator</a>**  | which authenticator scylla will use AllowAllAuthenticator/PasswordAuthenticator | N/A | SCT_AUTHENTICATOR
| **<a name="authenticator_user">authenticator_user</a>**  | the username if PasswordAuthenticator is used | N/A | SCT_AUTHENTICATOR_USER
| **<a name="authenticator_password">authenticator_password</a>**  | the password if PasswordAuthenticator is used | N/A | SCT_AUTHENTICATOR_PASSWORD
| **<a name="authorizer">authorizer</a>**  | which authorizer scylla will use AllowAllAuthorizer/CassandraAuthorizer | N/A | SCT_AUTHORIZER
| **<a name="system_auth_rf">system_auth_rf</a>**  | Replication factor will be set to system_auth | 3 | SCT_SYSTEM_AUTH_RF
| **<a name="alternator_port">alternator_port</a>**  | Port to configure for alternator in scylla.yaml | N/A | SCT_ALTERNATOR_PORT
| **<a name="dynamodb_primarykey_type">dynamodb_primarykey_type</a>**  | Type of dynamodb table to create with range key or not, can be HASH or HASH_AND_RANGE | N/A | SCT_DYNAMODB_PRIMARYKEY_TYPE
| **<a name="append_scylla_args">append_scylla_args</a>**  | More arguments to append to scylla command line | --blocked-reactor-notify-ms 500 --abort-on-lsa-bad-alloc 1 --abort-on-seastar-bad-alloc --abort-on-internal-error 1 | SCT_APPEND_SCYLLA_ARGS
| **<a name="append_scylla_args_oracle">append_scylla_args_oracle</a>**  | More arguments to append to oracle command line | N/A | SCT_APPEND_SCYLLA_ARGS_ORACLE
| **<a name="append_scylla_yaml">append_scylla_yaml</a>**  | More configuration to append to /etc/scylla/scylla.yaml | N/A | SCT_APPEND_SCYLLA_YAML
| **<a name="nemesis_class_name">nemesis_class_name</a>**  | Nemesis class to use (possible types in sdcm.nemesis).<br>Next syntax supporting:<br>- nemesis_class_name: "NemesisName"  Run one nemesis in single thread<br>- nemesis_class_name: "<NemesisName>:<num>" Run <NemesisName> in <num><br>parallel threads on different nodes. Ex.: "ChaosMonkey:2"<br>- nemesis_class_name: "<NemesisName1>:<num1> <NemesisName2>:<num2>" Run<br><NemesisName1> in <num1> parallel threads and <NemesisName2> in <num2><br>parallel threads. Ex.: "DisruptiveMonkey:1 NonDisruptiveMonkey:2" | NoOpMonkey | SCT_NEMESIS_CLASS_NAME
| **<a name="nemesis_interval">nemesis_interval</a>**  | Nemesis sleep interval to use if None provided specifically in the test | 5 | SCT_NEMESIS_INTERVAL
| **<a name="nemesis_during_prepare">nemesis_during_prepare</a>**  | Run nemesis during prepare stage of the test | True | SCT_NEMESIS_DURING_PREPARE
| **<a name="cluster_target_size">cluster_target_size</a>**  | Used for scale test: max size of the cluster | N/A | SCT_CLUSTER_TARGET_SIZE
| **<a name="space_node_threshold">space_node_threshold</a>**  | Space node threshold before starting nemesis (bytes)<br>The default value is 6GB (6x1024^3 bytes)<br>This value is supposed to reproduce<br>https://github.com/scylladb/scylla/issues/1140 | N/A | SCT_SPACE_NODE_THRESHOLD
| **<a name="nemesis_filter_seeds">nemesis_filter_seeds</a>**  | If true runs the nemesis only on non seed nodes | True | SCT_NEMESIS_FILTER_SEEDS
| **<a name="stress_cmd">stress_cmd</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD
| **<a name="gemini_version">gemini_version</a>**  | Version of download of the binaries of gemini tool | N/A | SCT_GEMINI_VERSION
| **<a name="gemini_schema_url">gemini_schema_url</a>**  | Url of the schema/configuration the gemini tool would use | N/A | SCT_GEMINI_SCHEMA_URL
| **<a name="gemini_cmd">gemini_cmd</a>**  | gemini command to run (for now used only in GeminiTest) | N/A | SCT_GEMINI_CMD
| **<a name="gemini_seed">gemini_seed</a>**  | Seed number for gemini command | N/A | SCT_GEMINI_SEED
| **<a name="instance_type_loader">instance_type_loader</a>**  | AWS image type of the loader node | N/A | SCT_INSTANCE_TYPE_LOADER
| **<a name="instance_type_monitor">instance_type_monitor</a>**  | AWS image type of the monitor node | N/A | SCT_INSTANCE_TYPE_MONITOR
| **<a name="instance_type_db">instance_type_db</a>**  | AWS image type of the db node | N/A | SCT_INSTANCE_TYPE_DB
| **<a name="instance_type_db_oracle">instance_type_db_oracle</a>**  | AWS image type of the oracle node | N/A | SCT_INSTANCE_TYPE_DB_ORACLE
| **<a name="region_name">region_name</a>**  | AWS regions to use | N/A | SCT_REGION_NAME
| **<a name="security_group_ids">security_group_ids</a>**  | AWS security groups ids to use | N/A | SCT_SECURITY_GROUP_IDS
| **<a name="subnet_id">subnet_id</a>**  | AWS subnet ids to use | N/A | SCT_SUBNET_ID
| **<a name="ami_id_db_scylla">ami_id_db_scylla</a>**  | AMS AMI id to use for scylla db node | N/A | SCT_AMI_ID_DB_SCYLLA
| **<a name="ami_id_loader">ami_id_loader</a>**  | AMS AMI id to use for loader node | N/A | SCT_AMI_ID_LOADER
| **<a name="ami_id_monitor">ami_id_monitor</a>**  | AMS AMI id to use for monitor node | N/A | SCT_AMI_ID_MONITOR
| **<a name="ami_id_db_cassandra">ami_id_db_cassandra</a>**  | AMS AMI id to use for cassandra node | N/A | SCT_AMI_ID_DB_CASSANDRA
| **<a name="ami_id_db_oracle">ami_id_db_oracle</a>**  | AMS AMI id to use for oracle node | N/A | SCT_AMI_ID_DB_ORACLE
| **<a name="aws_root_disk_size_db">aws_root_disk_size_db</a>**  |  | N/A | SCT_AWS_ROOT_DISK_SIZE_DB
| **<a name="aws_root_disk_name_db">aws_root_disk_name_db</a>**  |  | N/A | SCT_AWS_ROOT_DISK_NAME_DB
| **<a name="aws_root_disk_size_monitor">aws_root_disk_size_monitor</a>**  |  | N/A | SCT_AWS_ROOT_DISK_SIZE_MONITOR
| **<a name="aws_root_disk_name_monitor">aws_root_disk_name_monitor</a>**  |  | N/A | SCT_AWS_ROOT_DISK_NAME_MONITOR
| **<a name="aws_root_disk_size_loader">aws_root_disk_size_loader</a>**  |  | N/A | SCT_AWS_ROOT_DISK_SIZE_MONITOR
| **<a name="aws_root_disk_name_loader">aws_root_disk_name_loader</a>**  |  | N/A | SCT_AWS_ROOT_DISK_SIZE_MONITOR
| **<a name="ami_db_scylla_user">ami_db_scylla_user</a>**  |  | N/A | SCT_AMI_DB_SCYLLA_USER
| **<a name="ami_monitor_user">ami_monitor_user</a>**  |  | N/A | SCT_AMI_MONITOR_USER
| **<a name="ami_loader_user">ami_loader_user</a>**  |  | N/A | SCT_AMI_LOADER_USER
| **<a name="ami_db_cassandra_user">ami_db_cassandra_user</a>**  |  | N/A | SCT_AMI_DB_CASSANDRA_USER
| **<a name="spot_max_price">spot_max_price</a>**  | The max percentage of the on demand price we set for spot/fleet instances | 0.6 | SCT_SPOT_MAX_PRICE
| **<a name="extra_network_interface">extra_network_interface</a>**  | if true, create extra network interface on each node | N/A | SCT_EXTRA_NETWORK_INTERFACE
| **<a name="aws_instance_profile_name">aws_instance_profile_name</a>**  | This is the name of the instance profile to set on all instances | N/A | SCT_AWS_INSTANCE_PROFILE_NAME
| **<a name="backup_bucket_location">backup_bucket_location</a>**  | This is the bucket name to be used for backup with its region<br>(e.g. backup_bucket_location: 'manager-backup-tests') | N/A | SCT_BACKUP_BUCKET_LOCATION
| **<a name="tag_ami_with_result">tag_ami_with_result</a>**  | If True, would tag the ami with the test final result | N/A | SCT_TAG_AMI_WITH_RESULT
| **<a name="gce_datacenter">gce_datacenter</a>**  |  | N/A | SCT_GCE_DATACENTER
| **<a name="gce_network">gce_network</a>**  |  | N/A | SCT_GCE_DATACENTER
| **<a name="gce_image">gce_image</a>**  |  | N/A | SCT_GCE_IMAGE
| **<a name="gce_image_db">gce_image_db</a>**  |  | N/A | SCT_GCE_IMAGE_DB
| **<a name="gce_image_monitor">gce_image_monitor</a>**  |  | N/A | SCT_GCE_IMAGE_MONITOR
| **<a name="gce_image_username">gce_image_username</a>**  |  | N/A | SCT_GCE_IMAGE_USERNAME
| **<a name="gce_instance_type_loader">gce_instance_type_loader</a>**  |  | N/A | SCT_GCE_INSTANCE_TYPE_LOADER
| **<a name="gce_root_disk_type_loader">gce_root_disk_type_loader</a>**  |  | N/A | SCT_GCE_ROOT_DISK_TYPE_LOADER
| **<a name="gce_n_local_ssd_disk_loader">gce_n_local_ssd_disk_loader</a>**  |  | N/A | SCT_GCE_N_LOCAL_SSD_DISK_LOADER
| **<a name="gce_instance_type_monitor">gce_instance_type_monitor</a>**  |  | N/A | SCT_GCE_INSTANCE_TYPE_MONITOR
| **<a name="gce_root_disk_type_monitor">gce_root_disk_type_monitor</a>**  |  | N/A | SCT_GCE_ROOT_DISK_TYPE_MONITOR
| **<a name="gce_root_disk_size_monitor">gce_root_disk_size_monitor</a>**  |  | N/A | SCT_GCE_ROOT_DISK_SIZE_MONITOR
| **<a name="gce_n_local_ssd_disk_monitor">gce_n_local_ssd_disk_monitor</a>**  |  | N/A | SCT_GCE_N_LOCAL_SSD_DISK_MONITOR
| **<a name="gce_instance_type_db">gce_instance_type_db</a>**  |  | N/A | SCT_GCE_INSTANCE_TYPE_DB
| **<a name="gce_root_disk_type_db">gce_root_disk_type_db</a>**  |  | N/A | SCT_GCE_ROOT_DISK_TYPE_DB
| **<a name="gce_root_disk_size_db">gce_root_disk_size_db</a>**  |  | N/A | SCT_GCE_ROOT_DISK_SIZE_DB
| **<a name="gce_n_local_ssd_disk_db">gce_n_local_ssd_disk_db</a>**  |  | N/A | SCT_GCE_N_LOCAL_SSD_DISK_DB
| **<a name="docker_image">docker_image</a>**  |  | N/A | SCT_DOCKER_IMAGE
| **<a name="db_nodes_private_ip">db_nodes_private_ip</a>**  |  | N/A | SCT_DB_NODES_PRIVATE_IP
| **<a name="db_nodes_public_ip">db_nodes_public_ip</a>**  |  | N/A | SCT_DB_NODES_PUBLIC_IP
| **<a name="loaders_private_ip">loaders_private_ip</a>**  |  | N/A | SCT_LOADERS_PRIVATE_IP
| **<a name="loaders_public_ip">loaders_public_ip</a>**  |  | N/A | SCT_LOADERS_PUBLIC_IP
| **<a name="monitor_nodes_private_ip">monitor_nodes_private_ip</a>**  |  | N/A | SCT_MONITOR_NODES_PRIVATE_IP
| **<a name="monitor_nodes_public_ip">monitor_nodes_public_ip</a>**  |  | N/A | SCT_MONITOR_NODES_PUBLIC_IP
| **<a name="cassandra_stress_population_size">cassandra_stress_population_size</a>**  |  | N/A | SCT_CASSANDRA_STRESS_POPULATION_SIZE
| **<a name="cassandra_stress_threads">cassandra_stress_threads</a>**  |  | N/A | SCT_CASSANDRA_STRESS_THREADS
| **<a name="add_node_cnt">add_node_cnt</a>**  |  | N/A | SCT_ADD_NODE_CNT
| **<a name="stress_multiplier">stress_multiplier</a>**  |  | N/A | SCT_STRESS_MULTIPLIER
| **<a name="run_fullscan">run_fullscan</a>**  |  | N/A | SCT_RUN_FULLSCAN
| **<a name="keyspace_num">keyspace_num</a>**  |  | N/A | SCT_KEYSPACE_NUM
| **<a name="round_robin">round_robin</a>**  |  | N/A | SCT_ROUND_ROBIN
| **<a name="batch_size">batch_size</a>**  |  | N/A | SCT_BATCH_SIZE
| **<a name="pre_create_schema">pre_create_schema</a>**  |  | N/A | SCT_PRE_CREATE_SCHEMA
| **<a name="compaction_strategy">compaction_strategy</a>**  | Choose a specific compaction strategy to pre-create schema with. | N/A | SCT_COMPACTION_STRATEGY
| **<a name="cluster_health_check">cluster_health_check</a>**  | When true, start cluster health checker for all nodes | True | SCT_CLUSTER_HEALTH_CHECK
| **<a name="validate_partitions">validate_partitions</a>**  | when true, log of the partitions before and after the nemesis run is compacted | N/A | SCT_VALIDATE_PARTITIONS
| **<a name="table_name">table_name</a>**  | table name to check for the validate_partitions check | N/A | SCT_TABLE_NAME
| **<a name="primary_key_column">primary_key_column</a>**  | primary key of the table to check for the validate_partitions check | N/A | SCT_PRIMARY_KEY_COLUMN
| **<a name="stress_read_cmd">stress_read_cmd</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_READ_CMD
| **<a name="prepare_verify_cmd">prepare_verify_cmd</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_PREPARE_VERIFY_CMD
| **<a name="user_profile_table_count">user_profile_table_count</a>**  | number of tables to create for template user c-s | N/A | SCT_USER_PROFILE_TABLE_COUNT
| **<a name="scylla_mgmt_upgrade_to_repo">scylla_mgmt_upgrade_to_repo</a>**  | Url to the repo of scylla manager version to upgrade to for management tests | N/A | SCT_SCYLLA_MGMT_UPGRADE_TO_REPO
| **<a name="stress_cmd_w">stress_cmd_w</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_W
| **<a name="stress_cmd_r">stress_cmd_r</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_R
| **<a name="stress_cmd_m">stress_cmd_m</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_M
| **<a name="prepare_write_cmd">prepare_write_cmd</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_PREPARE_WRITE_CMD
| **<a name="stress_cmd_no_mv">stress_cmd_no_mv</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_NO_MV
| **<a name="stress_cmd_no_mv_profile">stress_cmd_no_mv_profile</a>**  |  | N/A | SCT_STRESS_CMD_NO_MV_PROFILE
| **<a name="cs_user_profiles">cs_user_profiles</a>**  |  | N/A | SCT_CS_USER_PROFILES
| **<a name="cs_duration">cs_duration</a>**  |  | N/A | SCT_CS_DURATION
| **<a name="stress_cmd_mv">stress_cmd_mv</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_MV
| **<a name="prepare_stress_cmd">prepare_stress_cmd</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_PREPARE_STRESS_CMD
| **<a name="skip_download">skip_download</a>**  |  | N/A | SCT_SKIP_DOWNLOAD
| **<a name="sstable_file">sstable_file</a>**  |  | N/A | SCT_SSTABLE_FILE
| **<a name="sstable_url">sstable_url</a>**  |  | N/A | SCT_SSTABLE_URL
| **<a name="sstable_md5">sstable_md5</a>**  |  | N/A | SCT_SSTABLE_MD5
| **<a name="flush_times">flush_times</a>**  |  | N/A | SCT_FLUSH_TIMES
| **<a name="flush_period">flush_period</a>**  |  | N/A | SCT_FLUSH_PERIOD
| **<a name="new_scylla_repo">new_scylla_repo</a>**  |  | N/A | SCT_NEW_SCYLLA_REPO
| **<a name="new_version">new_version</a>**  | Assign new upgrade version, use it to upgrade to specific minor release. eg: 3.0.1 | N/A | SCT_NEW_VERSION
| **<a name="target_upgrade_version">target_upgrade_version</a>**  | Assign target upgrade version, use for decide if the truncate entries test should be run. This test should be performed in case the target upgrade version >= 3.1 | N/A | SCT_TAGRET_UPGRADE_VERSION
| **<a name="upgrade_node_packages">upgrade_node_packages</a>**  |  | N/A | SCT_UPGRADE_NODE_PACKAGES
| **<a name="test_sst3">test_sst3</a>**  |  | N/A | SCT_TEST_SST3
| **<a name="test_upgrade_from_installed_3_1_0">test_upgrade_from_installed_3_1_0</a>**  | Enable an option for installed 3.1.0 for work around a scylla issue if it's true | N/A | SCT_TEST_UPGRADE_FROM_INSTALLED_3_1_0
| **<a name="authorization_in_upgrade">authorization_in_upgrade</a>**  | Which Authorization to enable after upgrade | N/A | SCT_AUTHORIZATION_IN_UPGRADE
| **<a name="remove_authorization_in_rollback">remove_authorization_in_rollback</a>**  | Disable Authorization after rollback to old Scylla | N/A | SCT_REMOVE_AUTHORIZATION_IN_ROLLBACK
| **<a name="new_introduced_pkgs">new_introduced_pkgs</a>**  |  | N/A | SCT_NEW_INTRODUCED_PKGS
| **<a name="recover_system_tables">recover_system_tables</a>**  |  | N/A | SCT_RECOVER_SYSTEM_TABLES
| **<a name="stress_cmd_1">stress_cmd_1</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_1
| **<a name="stress_cmd_complex_prepare">stress_cmd_complex_prepare</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_COMPLEX_PREPARE
| **<a name="prepare_write_stress">prepare_write_stress</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_PREPARE_WRITE_STRESS
| **<a name="stress_cmd_read_10m">stress_cmd_read_10m</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_READ_10M
| **<a name="stress_cmd_read_cl_one">stress_cmd_read_cl_one</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure. | N/A | SCT_STRESS_CMD_READ_CL_ONE
| **<a name="stress_cmd_read_80m">stress_cmd_read_80m</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_READ_80M
| **<a name="stress_cmd_complex_verify_read">stress_cmd_complex_verify_read</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_COMPLEX_VERIFY_READ
| **<a name="stress_cmd_complex_verify_more">stress_cmd_complex_verify_more</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_COMPLEX_VERIFY_MORE
| **<a name="write_stress_during_entire_test">write_stress_during_entire_test</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_WRITE_STRESS_DURING_ENTIRE_TEST
| **<a name="verify_data_after_entire_test">verify_data_after_entire_test</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure. | N/A | SCT_VERIFY_DATA_AFTER_ENTIRE_TEST
| **<a name="stress_cmd_read_cl_quorum">stress_cmd_read_cl_quorum</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_READ_CL_QUORUM
| **<a name="verify_stress_after_cluster_upgrade">verify_stress_after_cluster_upgrade</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_VERIFY_STRESS_AFTER_CLUSTER_UPGRADE
| **<a name="stress_cmd_complex_verify_delete">stress_cmd_complex_verify_delete</a>**  | cassandra-stress commands.<br>You can specify everything but the -node parameter, which is going to<br>be provided by the test suite infrastructure.<br>multiple commands can passed as a list | N/A | SCT_STRESS_CMD_COMPLEX_VERIFY_DELETE
| **<a name="scylla_encryption_options">scylla_encryption_options</a>**  | options will be used for enable encryption at-rest for tables | N/A | SCT_SCYLLA_ENCRYPTION_OPTIONS
| **<a name="logs_transport">logs_transport</a>**  | How to transport logs: rsyslog or ssh | rsyslog | SCT_LOGS_TRANSPORT
| **<a name="collect_logs">collect_logs</a>**  | Collect logs from instances and sct runner | N/A | SCT_COLLECT_LOGS
| **<a name="execute_post_behavior">execute_post_behavior</a>**  | Run post behavior actions in sct teardown step | N/A | SCT_EXECUTE_POST_BEHAVIOR
| **<a name="post_behavior_db_nodes">post_behavior_db_nodes</a>**  | Failure/post test behavior, i.e. what to do with the db cloud instances at the end of the test.<br><br>'destroy' - Destroy instances and credentials (default)<br>'keep' - Keep instances running and leave credentials alone<br>'keep-on-failure' - Keep instances if testrun failed | keep-on-failure | SCT_POST_BEHAVIOR_DB_NODES
| **<a name="post_behavior_loader_nodes">post_behavior_loader_nodes</a>**  | Failure/post test behavior, i.e. what to do with the loader cloud instances at the end of the test.<br><br>'destroy' - Destroy instances and credentials (default)<br>'keep' - Keep instances running and leave credentials alone<br>'keep-on-failure' - Keep instances if testrun failed | destroy | SCT_POST_BEHAVIOR_LOADER_NODES
| **<a name="post_behavior_monitor_nodes">post_behavior_monitor_nodes</a>**  | Failure/post test behavior, i.e. what to do with the monitor cloud instances at the end of the test.<br><br>'destroy' - Destroy instances and credentials (default)<br>'keep' - Keep instances running and leave credentials alone<br>'keep-on-failure' - Keep instances if testrun failed | keep-on-failure | SCT_POST_BEHAVIOR_MONITOR_NODES
| **<a name="workaround_kernel_bug_for_iotune">workaround_kernel_bug_for_iotune</a>**  | Workaround a known kernel bug which causes iotune to fail in scylla_io_setup, only effect GCE backend | N/A | SCT_WORKAROUND_KERNEL_BUG_FOR_IOTUNE
| **<a name="loader_swap_size">loader_swap_size</a>**  | The size of the swap file for the loaders. Its size in bytes calculated by x * 1MB | 1024 | SCT_LOADER_SWAP_SIZE
| **<a name="monitor_swap_size">monitor_swap_size</a>**  | The size of the swap file for the monitors. Its size in bytes calculated by x * 1MB | 8192 | SCT_MONITOR_SWAP_SIZE
| **<a name="store_perf_results">store_perf_results</a>**  | A flag that indicates whether or not to gather the prometheus stats at the end of the run.<br>Intended to be used in performance testing | N/A | SCT_STORE_PERF_RESULTS
